{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO7TxzlbnAj7+7lJBruHyf5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bangjaia/Keras/blob/main/%ED%95%84%EA%B8%B0%EC%B2%B4_%EC%83%9D%EC%84%B1_%ED%95%A9%EC%84%B1%EA%B3%B1_%EA%B3%84%EC%B8%B5_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFXCljD2RcqE",
        "outputId": "fd42df8f-de95-4476-d4da-ee1715e60ed1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function image_data_format at 0x7d29d4368220>\n",
            "Output_fold is GAN_OUT\n",
            "Epoch is 0\n",
            "Number of batches 2\n",
            "Epoch is 10\n",
            "Number of batches 2\n",
            "Epoch is 20\n",
            "Number of batches 2\n",
            "Epoch is 30\n",
            "Number of batches 2\n",
            "Epoch is 40\n",
            "Number of batches 2\n",
            "Epoch is 50\n",
            "Number of batches 2\n",
            "Epoch is 60\n",
            "Number of batches 2\n",
            "Epoch is 70\n",
            "Number of batches 2\n",
            "Epoch is 80\n",
            "Number of batches 2\n",
            "Epoch is 90\n",
            "Number of batches 2\n",
            "Epoch is 100\n",
            "Number of batches 2\n",
            "Epoch is 110\n",
            "Number of batches 2\n",
            "Epoch is 120\n",
            "Number of batches 2\n",
            "Epoch is 130\n",
            "Number of batches 2\n",
            "Epoch is 140\n",
            "Number of batches 2\n",
            "Epoch is 150\n",
            "Number of batches 2\n",
            "Epoch is 160\n",
            "Number of batches 2\n",
            "Epoch is 170\n",
            "Number of batches 2\n",
            "Epoch is 180\n",
            "Number of batches 2\n",
            "Epoch is 190\n",
            "Number of batches 2\n",
            "Epoch is 200\n",
            "Number of batches 2\n",
            "Epoch is 210\n",
            "Number of batches 2\n",
            "Epoch is 220\n",
            "Number of batches 2\n",
            "Epoch is 230\n",
            "Number of batches 2\n",
            "Epoch is 240\n",
            "Number of batches 2\n",
            "Epoch is 250\n",
            "Number of batches 2\n",
            "Epoch is 260\n",
            "Number of batches 2\n",
            "Epoch is 270\n",
            "Number of batches 2\n",
            "Epoch is 280\n",
            "Number of batches 2\n",
            "Epoch is 290\n",
            "Number of batches 2\n",
            "Epoch is 300\n",
            "Number of batches 2\n",
            "Epoch is 310\n",
            "Number of batches 2\n",
            "Epoch is 320\n",
            "Number of batches 2\n",
            "Epoch is 330\n",
            "Number of batches 2\n",
            "Epoch is 340\n",
            "Number of batches 2\n",
            "Epoch is 350\n",
            "Number of batches 2\n",
            "Epoch is 360\n",
            "Number of batches 2\n",
            "Epoch is 370\n",
            "Number of batches 2\n",
            "Epoch is 380\n",
            "Number of batches 2\n",
            "Epoch is 390\n",
            "Number of batches 2\n",
            "Epoch is 400\n",
            "Number of batches 2\n",
            "Epoch is 410\n",
            "Number of batches 2\n",
            "Epoch is 420\n",
            "Number of batches 2\n",
            "Epoch is 430\n",
            "Number of batches 2\n",
            "Epoch is 440\n",
            "Number of batches 2\n",
            "Epoch is 450\n",
            "Number of batches 2\n",
            "Epoch is 460\n",
            "Number of batches 2\n",
            "Epoch is 470\n",
            "Number of batches 2\n",
            "Epoch is 480\n",
            "Number of batches 2\n",
            "Epoch is 490\n",
            "Number of batches 2\n",
            "Epoch is 500\n",
            "Number of batches 2\n",
            "Epoch is 510\n",
            "Number of batches 2\n",
            "Epoch is 520\n",
            "Number of batches 2\n",
            "Epoch is 530\n",
            "Number of batches 2\n",
            "Epoch is 540\n",
            "Number of batches 2\n",
            "Epoch is 550\n",
            "Number of batches 2\n",
            "Epoch is 560\n",
            "Number of batches 2\n",
            "Epoch is 570\n",
            "Number of batches 2\n",
            "Epoch is 580\n",
            "Number of batches 2\n",
            "Epoch is 590\n",
            "Number of batches 2\n",
            "Epoch is 600\n",
            "Number of batches 2\n",
            "Epoch is 610\n",
            "Number of batches 2\n",
            "Epoch is 620\n",
            "Number of batches 2\n",
            "Epoch is 630\n",
            "Number of batches 2\n",
            "Epoch is 640\n",
            "Number of batches 2\n",
            "Epoch is 650\n",
            "Number of batches 2\n",
            "Epoch is 660\n",
            "Number of batches 2\n",
            "Epoch is 670\n",
            "Number of batches 2\n",
            "Epoch is 680\n",
            "Number of batches 2\n",
            "Epoch is 690\n",
            "Number of batches 2\n",
            "Epoch is 700\n",
            "Number of batches 2\n",
            "Epoch is 710\n",
            "Number of batches 2\n",
            "Epoch is 720\n",
            "Number of batches 2\n",
            "Epoch is 730\n",
            "Number of batches 2\n",
            "Epoch is 740\n",
            "Number of batches 2\n",
            "Epoch is 750\n",
            "Number of batches 2\n",
            "Epoch is 760\n",
            "Number of batches 2\n",
            "Epoch is 770\n",
            "Number of batches 2\n",
            "Epoch is 780\n",
            "Number of batches 2\n",
            "Epoch is 790\n",
            "Number of batches 2\n",
            "Epoch is 800\n",
            "Number of batches 2\n",
            "Epoch is 810\n",
            "Number of batches 2\n",
            "Epoch is 820\n",
            "Number of batches 2\n",
            "Epoch is 830\n",
            "Number of batches 2\n",
            "Epoch is 840\n",
            "Number of batches 2\n",
            "Epoch is 850\n",
            "Number of batches 2\n",
            "Epoch is 860\n",
            "Number of batches 2\n",
            "Epoch is 870\n",
            "Number of batches 2\n",
            "Epoch is 880\n",
            "Number of batches 2\n",
            "Epoch is 890\n",
            "Number of batches 2\n",
            "Epoch is 900\n",
            "Number of batches 2\n",
            "Epoch is 910\n",
            "Number of batches 2\n",
            "Epoch is 920\n",
            "Number of batches 2\n",
            "Epoch is 930\n",
            "Number of batches 2\n",
            "Epoch is 940\n",
            "Number of batches 2\n",
            "Epoch is 950\n",
            "Number of batches 2\n",
            "Epoch is 960\n",
            "Number of batches 2\n",
            "Epoch is 970\n",
            "Number of batches 2\n",
            "Epoch is 980\n",
            "Number of batches 2\n",
            "Epoch is 990\n",
            "Number of batches 2\n"
          ]
        }
      ],
      "source": [
        "################################\n",
        "# 공통 패키지 불러오기\n",
        "################################\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import math\n",
        "import os\n",
        "\n",
        "from keras import models, layers, optimizers\n",
        "from keras.datasets import mnist\n",
        "import keras.backend as K\n",
        "\n",
        "print(K.image_data_format)\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def mse_4d(y_true, y_pred):\n",
        "    return K.mean(K.square(y_pred - y_true), axis=(1,2,3))\n",
        "\n",
        "def mse_4d_tf(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_pred - y_true), axis=(1,2,3))\n",
        "\n",
        "################################\n",
        "# GAN 모델링\n",
        "################################\n",
        "class GAN(models.Sequential):\n",
        "    def __init__(self, input_dim=62): # input_dim = args.n_train = 32\n",
        "        \"\"\"\n",
        "        self, self.generator, self.discriminator are all models\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        self.generator = self.GENERATOR()\n",
        "        self.discriminator = self.DISCRIMINATOR()\n",
        "        self.add(self.generator)\n",
        "        self.discriminator.trainable = False\n",
        "        self.add(self.discriminator)\n",
        "\n",
        "        self.compile_all()\n",
        "\n",
        "    def compile_all(self):\n",
        "        # Compiling stage\n",
        "        \"\"\"\n",
        "        lr의 경우 Keras3 이상 버전부터 사용되지 않고 대신 learning_rate가 사용됨.\n",
        "\n",
        "        그 외의 변경된 문법\n",
        "        lr\t                            learning_rate\n",
        "        decay\t                          weight_decay\n",
        "        keras.optimizers.Adam(lr=...)\t  keras.optimizers.Adam(learning_rate=...)\n",
        "        model.fit(x, y, nb_epoch=...)\t  model.fit(x, y, epochs=...)\n",
        "        현재코드에선 lr만 변경\n",
        "        \"\"\"\n",
        "        d_optim = optimizers.SGD(learning_rate=0.0005, momentum=0.9, nesterov=True)\n",
        "        g_optim = optimizers.SGD(learning_rate=0.0005, momentum=0.9, nesterov=True)\n",
        "        self.generator.compile(loss=mse_4d_tf, optimizer=\"SGD\")\n",
        "        self.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
        "        self.discriminator.trainable = True\n",
        "        self.discriminator.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
        "\n",
        "    def GENERATOR(self):\n",
        "        input_dim = self.input_dim\n",
        "\n",
        "        model = models.Sequential()\n",
        "        model.add(layers.Dense(1024, activation='tanh', input_dim=input_dim))\n",
        "        model.add(layers.Dense(7 * 7 * 128, activation='tanh')) # H, W, C = 7, 7, 128\n",
        "        model.add(layers.BatchNormalization())\n",
        "        # The Conv2D op currently only supports the NHWC tensor format on the CPU.\n",
        "        model.add(layers.Reshape((7, 7, 128), input_shape=(7 * 7 * 128,)))\n",
        "        model.add(layers.UpSampling2D(size=(2, 2)))\n",
        "        model.add(layers.Conv2D(64, (5, 5), padding='same', activation='tanh'))\n",
        "        model.add(layers.UpSampling2D(size=(2, 2)))\n",
        "        model.add(layers.Conv2D(1, (5, 5), padding='same', activation='tanh'))\n",
        "        return model\n",
        "\n",
        "    def DISCRIMINATOR(self):\n",
        "        # The Conv2D op currently only supports the NHWC tensor format on the CPU.\n",
        "        model = models.Sequential()\n",
        "        model.add(layers.Conv2D(64, (5, 5), padding='same', activation='tanh',\n",
        "                                input_shape=(28, 28, 1)))\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(layers.Conv2D(128, (5, 5), activation='tanh'))\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(layers.Flatten())\n",
        "        model.add(layers.Dense(1024, activation='tanh'))\n",
        "        model.add(layers.Dense(1, activation='sigmoid'))\n",
        "        return model\n",
        "\n",
        "    def get_z(self, ln):\n",
        "        input_dim = self.input_dim\n",
        "        return np.random.uniform(-1, 1, (ln, input_dim))\n",
        "\n",
        "    def train_both(self, x):\n",
        "        ln = x.shape[0]\n",
        "        # First trial for training discriminator\n",
        "        z = self.get_z(ln)\n",
        "        w = self.generator.predict(z, verbose=0)\n",
        "        xw = np.concatenate((x, w))\n",
        "        y2 = np.array([1] * ln + [0] * ln).reshape(-1,1) # Necessary!\n",
        "        d_loss = self.discriminator.train_on_batch(xw, y2)\n",
        "\n",
        "        # Second trial for training generator\n",
        "        z = self.get_z(ln)\n",
        "        self.discriminator.trainable = False\n",
        "        g_loss = self.train_on_batch(z, np.array([1] * ln).reshape(-1, 1))\n",
        "        self.discriminator.trainable = True\n",
        "\n",
        "        return d_loss, g_loss\n",
        "\n",
        "################################\n",
        "# GAN 학습하기\n",
        "################################\n",
        "def combine_images(generated_images):\n",
        "    num = generated_images.shape[0]\n",
        "    width = int(math.sqrt(num))\n",
        "    height = int(math.ceil(float(num) / width))\n",
        "    shape = generated_images.shape[1:3] # (1,2) for NHWC\n",
        "    image = np.zeros((height * shape[0], width * shape[1]),\n",
        "                     dtype=generated_images.dtype)\n",
        "    for index, img in enumerate(generated_images):\n",
        "        i = int(index / width)\n",
        "        j = index % width\n",
        "        image[i * shape[0]:(i + 1) * shape[0],\n",
        "        j * shape[1]:(j + 1) * shape[1]] = img[ :, :, 0] # NHWC\n",
        "    return image\n",
        "\n",
        "def get_x(X_train, index, BATCH_SIZE):\n",
        "    return X_train[index * BATCH_SIZE:(index + 1) * BATCH_SIZE]\n",
        "\n",
        "def save_images(generated_images, output_fold, epoch, index):\n",
        "    image = combine_images(generated_images)\n",
        "    image = image * 127.5 + 127.5\n",
        "    Image.fromarray(image.astype(np.uint8)).save(\n",
        "        output_fold + '/' +\n",
        "        str(epoch) + \"_\" + str(index) + \".png\")\n",
        "\n",
        "def load_data(n_train):\n",
        "    (X_train, y_train), (_, _) = mnist.load_data()\n",
        "    return X_train[:n_train]\n",
        "\n",
        "def train(args):\n",
        "    BATCH_SIZE = args.batch_size\n",
        "    epochs = args.epochs\n",
        "    output_fold = args.output_fold\n",
        "    input_dim = args.input_dim\n",
        "    n_train = args.n_train\n",
        "\n",
        "    os.makedirs(output_fold, exist_ok=True)\n",
        "    print('Output_fold is', output_fold)\n",
        "\n",
        "    X_train = load_data(n_train)\n",
        "\n",
        "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "    # The Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW\n",
        "    # X_train = X_train.reshape((X_train.shape[0], 1) + X_train.shape[1:]) # <-- NCHW format\n",
        "    X_train = X_train.reshape(X_train.shape + (1,)) # <-- NHWC format\n",
        "\n",
        "    gan = GAN(input_dim)\n",
        "\n",
        "    d_loss_ll = []\n",
        "    g_loss_ll = []\n",
        "    for epoch in range(epochs):\n",
        "        if epoch % 10 == 0:\n",
        "            print(\"Epoch is\", epoch)\n",
        "            print(\"Number of batches\", int(X_train.shape[0] / BATCH_SIZE))\n",
        "\n",
        "        d_loss_l = []\n",
        "        g_loss_l = []\n",
        "        for index in range(int(X_train.shape[0] / BATCH_SIZE)):\n",
        "            x = get_x(X_train, index, BATCH_SIZE)\n",
        "\n",
        "            d_loss, g_loss = gan.train_both(x)\n",
        "\n",
        "            d_loss_l.append(d_loss)\n",
        "            g_loss_l.append(g_loss)\n",
        "\n",
        "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
        "            z = gan.get_z(x.shape[0])\n",
        "            w = gan.generator.predict(z, verbose=0)\n",
        "            save_images(w, output_fold, epoch, 0)\n",
        "\n",
        "        d_loss_ll.append(d_loss_l)\n",
        "        g_loss_ll.append(g_loss_l)\n",
        "\n",
        "    # gan.generator.save_weights(output_fold + '/' + 'generator', True)\n",
        "    # gan.discriminator.save_weights(output_fold + '/' + 'discriminator', True)\n",
        "\n",
        "    np.savetxt(output_fold + '/' + 'd_loss', d_loss_ll)\n",
        "    np.savetxt(output_fold + '/' + 'g_loss', g_loss_ll)\n",
        "\n",
        "################################\n",
        "# GAN 예제 실행하기\n",
        "################################\n",
        "import argparse\n",
        "import sys\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    \"\"\"Jupyter에서 커널 실행시 자동으로 \"-f\" 옵션이 추가됨.\n",
        "    argparse.ArgumentParser()에서는 해당 옵션 관련 처리가 없음.\n",
        "    따라서 \"-f\"를 모르는 인자로 인식하여 에러 발생\"\"\"\n",
        "\n",
        "    parser.add_argument('--batch_size', type=int, default=16,\n",
        "        help='Batch size for the networks')\n",
        "    parser.add_argument('--epochs', type=int, default=1000,\n",
        "        help='Epochs for the networks')\n",
        "    parser.add_argument('--output_fold', type=str, default='GAN_OUT',\n",
        "        help='Output fold to save the results')\n",
        "    parser.add_argument('--input_dim', type=int, default=10,\n",
        "        help='Input dimension for the generator.')\n",
        "    parser.add_argument('--n_train', type=int, default=32,\n",
        "        help='The number of training data.')\n",
        "\n",
        "    # args = parser.parse_args()\n",
        "    args, _ = parser.parse_known_args(sys.argv[1:]) #위의 Jupyter 인자를 무시하는 코드. 인식 못한 인자를 무시\n",
        "\n",
        "    train(args)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "neNlWmLqpi2a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}